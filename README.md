# ğŸ§¹ Task 1: Data Cleaning & Preprocessing â€“ AI & ML Internship

This repository contains my submission for Task 1 of the AI & ML Internship program. The objective was to clean and preprocess the **Titanic dataset** for machine learning.

---

## ğŸ“‚ Dataset

- Source: [Kaggle - Titanic Dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset)
- File used: `titanic.csv`

---

## âœ… Steps Performed

### 1. **Data Exploration**
- Used `pandas` to check data types, shape, null values, and statistical summary.

### 2. **Handling Missing Values**
- Imputed missing values in `Age` with median.
- Filled missing `Embarked` values with mode.
- Dropped the `Cabin` column due to excessive nulls.

### 3. **Encoding Categorical Variables**
- Applied one-hot encoding on `Sex` and `Embarked` columns using `pd.get_dummies()`.

### 4. **Feature Scaling**
- Normalized `Age` and `Fare` using `StandardScaler` from `sklearn`.

### 5. **Outlier Removal**
- Visualized outliers using `boxplots`.
- Removed extreme outliers in `Fare` above the 99th percentile.

---

## ğŸ“Š Tools Used

- Python
- Pandas
- NumPy
- Seaborn
- Matplotlib
- scikit-learn

---

## ğŸ“ Files in This Repo

- `titanic.csv` â€“ Original dataset (if allowed)
- `cleaned_titanic.csv` â€“ Final cleaned dataset
- `task1_preprocessing.ipynb` â€“ Jupyter notebook with full code
- `README.md` â€“ This documentation

---

## ğŸš€ How to Run

1. Clone this repository.
2. Open `task1_preprocessing.ipynb` in Jupyter Notebook.
3. Run all cells to see preprocessing in action.

---

## ğŸ“ Submission

[ğŸ”— Internship Submission Form](https://forms.gle/8Gm83s53KbyXs3Ne9)

---

## ğŸ’¡ Key Learnings

- Identified types of missing data and handled them appropriately.
- Understood the difference between normalization and standardization.
- Practiced encoding, outlier detection, and the impact of preprocessing on model performance.

---

Feel free to fork or star this repo if it helped you! â­
